<!DOCTYPE html>
<html lang="es">

<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-EVTW1307T1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-EVTW1307T1');
  </script>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Representaciones Semánticas y Deep Learning</title>
  <script src="../../index.js" defer></script>

  <!-- Mermaid -->
  <script type="module">
    import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs";
    mermaid.initialize({ startOnLoad: true });
  </script>

  <!-- HighlightJS -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>

  <script src="https://kit.fontawesome.com/a7cd9537ed.js" crossorigin="anonymous"></script>
  <script src="../index.js" defer></script>
  <script src="../menu.js" defer></script>
  <!-- <link rel="stylesheet" href="../index.css"> -->
  <link rel="stylesheet" href="../style.css" />
  <link rel="stylesheet" href="../menu.css">
</head>


<body>


  <nav id="vc-rail" class="vc-rail" aria-hidden="true">
    <div class="vc-rail-head">
      <span class="vc-rail-title">Índice</span>
      <button id="vc-close" class="vc-btn" aria-label="Cerrar menú">
        <!-- X SVG -->
        <svg width="18" height="18" viewBox="0 0 24 24" fill="none" aria-hidden="true">
          <path d="M6 6l12 12M18 6L6 18" stroke="currentColor" stroke-width="2" stroke-linecap="round" />
        </svg>
      </button>
    </div>
    <ul id="vc-list" class="vc-list"></ul>
  </nav>

  <div id="vc-backdrop" class="vc-backdrop" hidden></div>
  <!-- ===== /VC SIDEBAR ===== -->

  <header>

    <!-- ===== VC SIDEBAR (desde cero) ===== -->
    <button style="margin-top: 0!important;" id="vc-toggle" class="vc-toggle" aria-label="Mostrar/ocultar menú" aria-controls="vc-rail"
      aria-expanded="false">
      <!--Hamburger icon -->
      <i class="fa fa-bars" aria-hidden="true"></i>


    </button>
    <h1>Representaciones Semánticas y Deep Learning</h1>
    <img src="" id="logo" alt="Logo" style="display: none;">
  </header>

  <main>

    <section id="introduccion">
      <h2>Introducción</h2>

      <div class="diagram">
        <img src="../assets/deep_learning.webp" alt="Deep learning logo">
      </div>
      <p>
        En este tema aprenderemos cómo los modelos modernos representan el significado de las palabras
        y frases mediante vectores llamados <strong>embeddings</strong>. A diferencia de los
        métodos tradicionales, estos vectores capturan relaciones semánticas, permitiendo comparar
        textos según su <em>significado</em> y no solo por coincidencia de palabras.
      </p>

      <p>
        También veremos cómo calcular similitudes entre textos y cómo crear un sistema básico
        de búsqueda inteligente, similar al que utilizan los asistentes virtuales y los motores
        de recomendación basados en lenguaje.
      </p>


    </section>
    <section id="embeddings">
      <h2>1. ¿Qué son los embeddings?</h2>

      <p>
        Un <strong>embedding</strong> es una representación numérica de palabras o frases
        en forma de vector. La idea clave es que textos con significados parecidos tendrán
        vectores cercanos en el espacio, mientras que textos distintos estarán más alejados.
      </p>
      <div class="diagram">
        <img src="../assets/embedding-model.gif" alt="Embedding models diagram">
      </div>
      <p>
        Este tipo de representación permite que los modelos “entiendan” relaciones como:
      </p>

      <ul>
        <li>"gato" está cerca de "perro", pero lejos de "banana".</li>
        <li>"manzana" está cerca de "banana" y también de "google".</li>
        <li>"Madrid" estaría cerca de "Barcelona", pero lejos de "helado".</li>
      </ul>

      <div class="diagram">
        <img src="../assets/embedding-pair-vector.png" alt="Embedding pair vector diagram">
      </div>


      <div class="callout">
        Los embeddings capturan <strong>significado</strong>, no solo palabras.
      </div>
    </section>
    <section id="tipos-embeddings">
      <h2>2. Embeddings de palabras vs. embeddings de frases</h2>

      <p>
        Existen dos niveles de embeddings: los que representan palabras individuales y los que
        representan una frase completa. Los embeddings de frases suelen resultar más útiles
        para tareas reales, porque capturan el contexto completo.
      </p>

      <h3>2.1 Embeddings de palabras</h3>
      <ul>
        <li>Representan una sola palabra.</li>
        <li>Limitación: la palabra tiene el mismo vector siempre, aunque cambie su significado según el contexto.</li>
      </ul>

      <h3>2.2 Embeddings de frases</h3>
      <ul>
        <li>Capturan el significado de una frase entera.</li>
        <li>Se obtienen con modelos modernos basados en Transformers.</li>
      </ul>

      <table border="1" cellpadding="6" cellspacing="0">
        <caption><strong>Comparativa: Embeddings de frases vs. embeddings de palabras</strong></caption>
        <thead>
          <tr>
            <th>Característica</th>
            <th>Embeddings de frases</th>
            <th>Embeddings de palabras</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Alcance</td>
            <td>Representan frases completas (sentencias).</td>
            <td>Representan palabras individuales.</td>
          </tr>
          <tr>
            <td>Tipo de representación</td>
            <td>Capturan el significado global y el contexto de la frase.</td>
            <td>Capturan relaciones semánticas entre palabras individuales.</td>
          </tr>
          <tr>
            <td>Dependencia del contexto</td>
            <td>Dependientes del contexto (mismo término puede variar según la frase).</td>
            <td>Generalmente independientes del contexto (un vector por palabra).</td>
          </tr>
          <tr>
            <td>Uso típico</td>
            <td>Comprensión semántica a nivel de frase; comparación de textos completos.</td>
            <td>Analizar similitudes entre palabras; construir vocabularios y mapas semánticos.</td>
          </tr>
          <tr>
            <td>Aplicaciones</td>
            <td>Búsqueda semántica, recuperación de información, recomendación por significado.</td>
            <td>Tareas centradas en nivel palabra (analizar términos, analogías simples).</td>
          </tr>
          <tr>
            <td>Complejidad</td>
            <td>Más costosos computacionalmente (modelos de frases/Transformers).</td>
            <td>Menos costosos; se pueden entrenar o cargar rápidamente.</td>
          </tr>
          <tr>
            <td>Técnicas / modelos de ejemplo</td>
            <td>Sentence-BERT (SBERT), Universal Sentence Encoder (USE), modelos basados en Transformers.</td>
            <td>Word2Vec, GloVe, FastText.</td>
          </tr>
          <tr>
            <td>Ventajas</td>
            <td>Información rica y relevante al contexto; mejor para comparar significados completos.</td>
            <td>Eficientes para capturar relaciones a nivel palabra; útiles con vocabularios grandes.</td>
          </tr>
          <tr>
            <td>Desventajas</td>
            <td>Requieren más recursos y tiempo de cómputo.</td>
            <td>No capturan bien el contexto completo de una frase.</td>
          </tr>
        </tbody>
      </table>

      <div class="callout">
        Para tareas de clasificación, recomendación o búsqueda,
        los embeddings de frases suelen funcionar mejor.
      </div>
    </section>
    <section id="similitud">
  <h2>3. Similitud semántica</h2>

  <p>
    Una vez convertimos palabras o frases en vectores mediante <strong>embeddings</strong>,
    podemos comparar estos vectores entre sí. Esto nos permite saber si dos textos son
    parecidos en significado, incluso aunque no compartan exactamente las mismas palabras.
  </p>

  <p>
    Esta capacidad es fundamental en muchas aplicaciones modernas, como buscadores inteligentes,
    sistemas de recomendación, clasificación de documentos y chatbots que deben interpretar
    la intención del usuario.
  </p>

  <h3>3.1 ¿Por qué necesitamos medir similitud?</h3>
  <p>
    En los métodos tradicionales, comparar textos se basaba principalmente en contar palabras
    (como con BoW o TF‑IDF). Sin embargo, estos métodos fallan cuando dos textos usan palabras 
    distintas para expresar la misma idea. Los embeddings solucionan este problema representando 
    el <strong>significado</strong> en un espacio vectorial.
  </p>

  <h3>3.2 Similitud del coseno</h3>
  <p>
    La métrica más utilizada para comparar embeddings es la 
    <strong>similitud de coseno</strong>. El coseno mide el ángulo entre dos vectores:
  </p>

  <ul>
    <li><strong>Similitud = 1</strong> → vectores muy parecidos → textos muy similares.</li>
    <li><strong>Similitud ≈ 0</strong> → vectores perpendiculares → textos no relacionados.</li>
    <li><strong>Similitud = -1</strong> → significados opuestos (poco frecuente en PLN).</li>
  </ul>

 <div class="diagram">
        <img src="../assets/cosine similarity.png" alt="Cosine similarity diagram">
      </div>

  <p>
    El coseno se utiliza porque ignora la longitud del vector (su magnitud) y se centra
    únicamente en la dirección, que es lo que realmente representa el significado.
  </p>

  <h3>3.3 Ejemplo práctico</h3>
  <p>
    Supongamos que queremos comparar dos vectores generados por un modelo
    de embeddings. Con <code>cosine_similarity</code> podemos obtener rápidamente
    un valor entre 0 y 1 que indica su similitud.
  </p>

  <pre><code class="language-python">
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

v1 = np.array([1, 0, 1])
v2 = np.array([1, 1, 1])

print(cosine_similarity([v1], [v2]))
  </code></pre>

 
  <h3>3.4 ¿Qué aporta esto en la práctica?</h3>
  <p>
    Utilizar similitud semántica permite que un sistema encuentre textos relevantes
    aunque la consulta del usuario no coincida exactamente con las palabras del documento.
    Por ejemplo:
  </p>

  <ul>
    <li>“El paquete llegó roto” y “Mi pedido vino dañado” → alta similitud.</li>
    <li>“Me encanta este producto” y “No estoy satisfecho con la compra” → baja similitud.</li>
  </ul>

  <p>
    Esto hace posible crear buscadores inteligentes que trabajan por <strong>significado</strong>,
    no solo por palabra clave.
  </p>

  <div class="callout">
    <strong>Conclusión:</strong> la similitud semántica permite a los sistemas de IA
    relacionar textos según su significado y no según coincidencias superficiales de palabras.
    Es uno de los pilares del PLN moderno.
  </div>
</section>
    <!-- <section id="crear-embeddings">
      <h2>4. Crear embeddings con un modelo moderno</h2>

      <p>
        Hoy en día existen modelos especializados en generar embeddings de alta calidad.
        Su funcionamiento interno es complejo, pero utilizarlos es sencillo gracias a librerías
        como <strong>sentence-transformers</strong>.
      </p>

      <div class="diagram">
        <img src="../assets/sentence-transformers-logo.png" alt="Sentence Transformers logo">
      </div>
      <p>
        Sentence Transformers es una librería de Python que facilita el uso de modelos preentrenados
        para generar embeddings de frases. Estos modelos están basados en la arquitectura
        Transformer, que ha revolucionado el procesamiento del lenguaje natural.
      </p>
      
      <div class="callout">
        <strong>Documentación oficial:</strong> <a href="https://sbert.net/">https://sbert.net/</a>
      </div>

      <p>
        Para instalar la librería, usamos pip:
      </p>
      <pre><code class="language-bash">
pip install sentence-transformers
      </code></pre>
      
      <p>Ejemplo de uso de Sentence Transformers:</p>
      <pre><code class="language-python">
# Importamos la clase SentenceTransformer desde la librería sentence_transformers.
# Esta clase permite cargar modelos pre-entrenados para convertir texto en vectores numéricos (embeddings).
from sentence_transformers import SentenceTransformer

# Cargamos un modelo preentrenado. "all-MiniLM-L6-v2" es liviano y eficiente,
# ideal para generar embeddings semánticos.
modelo = SentenceTransformer("all-MiniLM-L6-v2")

# Lista de frases que queremos convertir en vectores.
frases = ["El producto llegó roto", "El envío fue muy rápido"]

# Generamos los embeddings. El resultado es una matriz donde cada fila
# representa el vector de una frase.
vectors = modelo.encode(frases)

# Imprimimos la forma de la matriz resultante.
# Debe mostrar (2, dimensiones) porque hay 2 frases,
# y cada una es convertida en un vector de cierta dimensión (p. ej., 384).
print(vectors.shape)
  </code></pre>

      

      <div class="callout">
        <strong>Nota:</strong> en clase no profundizamos en el modelo, solo en su uso práctico.
      </div>
    </section> -->
    <section id="crear-embeddings">
  <h2>4. Crear embeddings con un modelo moderno</h2>

  <p>
        Hoy en día existen modelos especializados en generar embeddings de alta calidad.
        Su funcionamiento interno es complejo, pero utilizarlos es sencillo gracias a librerías
        como <strong>sentence-transformers</strong>.
      </p>

      <div class="diagram">
        <img src="../assets/sentence-transformers-logo.png" alt="Sentence Transformers logo">
      </div>

  <p>
    Sentence Transformers es una librería de Python que facilita el uso de modelos preentrenados
    para generar embeddings de frases. Estos modelos están basados en la arquitectura
    Transformer, que ha revolucionado el procesamiento del lenguaje natural.
  </p>

  <p>
    Está basada en modelos tipo <strong>Transformer</strong>,
    muy eficientes a la hora de capturar significado. Con unas pocas líneas de código
    podemos convertir frases en vectores útiles para tareas reales como:
  </p>

  <ul>
    <li>búsqueda semántica (buscar por significado),</li>
    <li>detection de duplicados,</li>
    <li>clasificación de comentarios,</li>
    <li>agrupación automática de opiniones,</li>
    <li>sistemas de recomendación.</li>
  </ul>

  <div class="callout">
    <strong>Documentación oficial:</strong>
    <a href="https://sbert.net/">https://sbert.net/</a>
  </div>

  <p>Instalación mediante pip:</p>

  <pre><code class="language-bash">
pip install sentence-transformers
  </code></pre>

  <p>Ejemplo básico de uso:</p>

  <pre><code class="language-python">
# Importamos la clase SentenceTransformer desde la librería sentence_transformers.
# Sirve para cargar un modelo que convierte frases en vectores numéricos (embeddings).
from sentence_transformers import SentenceTransformer

# Cargamos un modelo preentrenado. "all-MiniLM-L6-v2" es rápido y produce
# embeddings de buena calidad que capturan el significado de las frases.
modelo = SentenceTransformer("all-MiniLM-L6-v2")

# Frases que queremos convertir en vectores.
frases = ["El producto llegó roto", "El envío fue muy rápido"]

# Convertimos cada frase en un vector. Obtendremos una matriz donde cada fila
# es el embedding de una frase.
vectors = modelo.encode(frases)

# Mostramos la forma (shape) de la matriz resultante:
# (2, dimensiones) significa: 2 frases, cada una convertida en un vector
# de varias dimensiones (normalmente 384).
print(vectors.shape)
  </code></pre>

  <p>
    Lo realmente útil de estos vectores no es ver sus números,
    sino <strong>compararlos</strong>. Si dos frases tienen significados parecidos,
    sus vectores estarán muy cerca entre sí. Con esto podemos construir sistemas de
    búsqueda inteligente, clasificación de opiniones, detección de duplicados, y mucho más.
  </p>

  <div class="callout">
    <strong>Nota:</strong> en clase nos centramos en el uso práctico, no en los detalles matemáticos o internos del modelo.
  </div>
</section>
    <section id="busqueda">
      <h2>5. Búsqueda inteligente basada en embeddings</h2>

      <p>
        La búsqueda tradicional compara palabras. Pero una búsqueda inteligente compara
        el <strong>significado</strong> usando embeddings y similitud de coseno.
      </p>

      <p>
        Esto permite encontrar resultados aunque no coincidan las palabras exactas.
      </p>

      <pre><code class="language-python"># Ejemplo de búsqueda semántica usando embeddings y similitud de coseno
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
import numpy as np
modelo = SentenceTransformer("all-MiniLM-L6-v2")
consulta = "paquete roto"
documentos = [
    "Mi pedido llegó dañado",
    "El envío fue muy rápido",
    "Estoy contento con el producto"
]

# Crear embeddings
vec_consulta = modelo.encode([consulta])
vec_docs = modelo.encode(documentos)

# Calcular similitudes
sim = cosine_similarity(vec_consulta, vec_docs)
print(sim)

umbral = 0.55
mejor = documentos[np.argmax(sim)]

if sim.max() < umbral:
    print("No hay coincidencias suficientemente parecidas.")
else:
    print("Mejor sentencia:", mejor)
  </code>
<strong>Resultado:</strong>
[[0.6053243  0.41458666 0.31157726]]
Mejor sentencia: Mi paquete llegó
</pre>

      <p><strong>Interpretación del resultado:</strong></p>
      <ul>
        <li>El valor más alto indica la frase más similar a la consulta.</li>
        <li>La similitud de coseno varía entre -1 y 1, siendo 1 la máxima similitud.</li>
        <li>Si la similitud máxima es menor que el umbral (0.55), no hay coincidencias relevantes.</li>
        <li>Entendemos que un valor por debajo de 0.55 indica que la similitud no es suficiente para considerarla relevante.</li>
      </ul>
      <p><strong>Con all-MiniLM-L6-v2, la escala suele ser:</strong></p>
      <ul>
        <li>0.7–1.0: muy similar</li>
        <li>0.5–0.7: moderadamente similar</li>
        <li>0.3–0.5: poco similar</li>
        <li>0.0–0.3: no relacionado</li>
      </ul>
      
      <div class="callout">
        Este método permite búsquedas más humanas y precisas.
      </div>
    </section>
    <section id="actividades">
  <h2>6. Actividades prácticas</h2>

  <p>
    A continuación se proponen varias actividades para practicar todo lo aprendido sobre
    embeddings, similitud semántica y búsqueda inteligente. Los ejercicios están pensados
    para que experimentes, observes diferencias entre modelos y comprendas cómo afectan
    los embeddings al funcionamiento de los sistemas modernos.
  </p>

  <h3>6.1 Actividad 1: Comparar frases</h3>
  <p>
    Genera embeddings de las siguientes frases y calcula la similitud de coseno entre
    cada par:
  </p>
  <ul>
    <li>“El paquete llegó roto”</li>
    <li>“El pedido vino dañado”</li>
    <li>“Estoy muy contento con mi compra”</li>
  </ul>
  <p>
    <strong>Pregunta:</strong> ¿Qué pareja de frases presenta mayor similitud? ¿Por qué?
  </p>

  <h3>6.2 Actividad 2: Cambiar el modelo</h3>
  <p>
    Repite la actividad anterior usando dos modelos distintos, por ejemplo:
  </p>
  <ul>
    <li><code>all-MiniLM-L6-v2</code></li>
    <li><code>all-mpnet-base-v2</code></li>
  </ul>
  <p>
    <strong>Preguntas:</strong>
  </p>
  <ul>
    <li>¿Qué modelo genera valores de similitud más altos?</li>
    <li>¿Cuál parece captar mejor el significado?</li>
    <li>¿Qué modelo usarías en un sistema real? ¿Por qué?</li>
  </ul>

  <h3>6.3 Actividad 3: Búsqueda semántica</h3>
  <p>
    Usando el código de búsqueda inteligente, añade los siguientes documentos:
  </p>
  <ul>
    <li>“El mensajero dejó el paquete en mal estado”</li>
    <li>“Todo llegó perfecto, gracias”</li>
    <li>“El envase venía abierto”</li>
  </ul>
  <p>
    Luego prueba estas consultas:
  </p>
  <ul>
    <li>“envío dañado”</li>
    <li>“muy satisfecho con el pedido”</li>
    <li>“problemas con el paquete”</li>
  </ul>
  <p>
    <strong>Preguntas:</strong>
  </p>
  <ul>
    <li>¿Qué documento devuelve el sistema para cada consulta?</li>
    <li>¿Hay casos en los que no encuentra nada relevante? ¿Por qué?</li>
  </ul>

  <h3>6.4 Actividad 4: Ajustar el umbral</h3>
  <p>
    Modifica el valor del umbral (<code>umbral = 0.55</code>) y repite la búsqueda.
    Prueba valores como 0.40, 0.60 o 0.70.
  </p>
  <p>
    <strong>Preguntas:</strong>
  </p>
  <ul>
    <li>¿Qué cambia cuando usas un umbral más alto?</li>
    <li>¿Qué riesgos tendría usar un umbral demasiado bajo o demasiado alto en un sistema real?</li>
  </ul>

  <h3>6.5 Actividad 5: Crear tu propio mini‑buscador</h3>
  <p>
    Escribe un pequeño programa que:
  </p>
  <ol>
    <li>Lea una consulta escrita por el usuario.</li>
    <li>Busque el documento más parecido usando embeddings.</li>
    <li>Devuelva el texto más relevante (o indique que no hay coincidencias).</li>
  </ol>
  
  <div class="callout">
    <strong>Recuerda:</strong> la mejor forma de entender los embeddings es usarlos,
    experimentar con distintos modelos y observar cómo afecta cada decisión al resultado.
  </div>
</section>


    <footer>
      <p>Profesor: Vicente Català — ®Todos los derechos reservados.</p>
    </footer>
  </main>
</body>



</html>